# Configuration for Fastest Provider Racing Proxy
# Races multiple downstream LLM APIs and returns the fastest successful response

# Server Configuration
server:
  host: "0.0.0.0"
  port: 8011

# Optional API Key for authentication
api_key: "123"

# Downstream API Configurations
downstream_apis:
  cerebras:
    base_url: "http://cerebras:8009/v1"
    api_key: "123"
    model: "gpt-oss-120b"
    stream: false

  groq:
    base_url: "http://groq:8010/v1"
    api_key: "123"
    model: "compound-beta"
    stream: false

  gemini:
    base_url: "http://gemini:8004/v1"
    api_key: "123"
    model: "gemini-2.5-pro"
    stream: false

  qwen:
    base_url: "http://qwen:8005/v1"
    api_key: "123"
    model: "qwen3-coder-plus"
    stream: false

  iflow:
    base_url: "http://iflow:8008/v1"
    api_key: "123"
    model: "kimi-k2-0905"
    stream: false

  openrouter:
    base_url: "http://openrouter:8006/v1"
    api_key: "123"
    model: "anthropic/claude-3.5-sonnet"
    stream: false

# Racing Configuration
racing:
  default_num_racers: 5      # Default number of APIs to race
  max_num_racers: 10         # Maximum number of APIs allowed to race

# Timeout Configuration
timeouts:
  http_client_timeout: 300.0

# Retry Configuration
retry:
  max_retries: 3
  base_delay: 1.0
  max_delay: 10.0
  jitter_range: 0.1
