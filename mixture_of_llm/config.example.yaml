# =============================================================================
# MOA (Mixture-of-Agents) Aggregator Configuration
# =============================================================================
# Copy this file to moa_config.yaml and customize for your deployment
# This service combines responses from multiple LLM providers for better quality


# =============================================================================
# DOWNSTREAM API CONFIGURATIONS
# =============================================================================
# Configure which LLM APIs to use as candidate generators
# Each entry sends the same prompt and generates candidate responses
downstream_apis:

  # --- GEMINI CANDIDATES ---
  gemini_candidate_1:
    base_url: "http://gemini:8004/v1"           # Internal Docker network URL
    api_key: "your-secret-api-key-here"         # Same as main config api_key
    model: "gemini-2.5-pro"                     # High quality, slower
    stream: false                                # Streaming not supported in MOA

  gemini_candidate_2:
    base_url: "http://gemini:8004/v1"
    api_key: "your-secret-api-key-here"
    model: "gemini-2.5-flash"                   # Faster alternative
    stream: false

  # --- QWEN CANDIDATES ---
  qwen_candidate_1:
    base_url: "http://qwen:8005/v1"
    api_key: "your-secret-api-key-here"
    model: "qwen3-coder-plus"                   # Best for coding tasks
    stream: false

  qwen_candidate_2:
    base_url: "http://qwen:8005/v1"
    api_key: "your-secret-api-key-here"
    model: "qwen3-coder-plus"                   # Duplicate for racing
    stream: false

  qwen_candidate_3:
    base_url: "http://qwen:8005/v1"
    api_key: "your-secret-api-key-here"
    model: "qwen3-coder-plus"                   # More candidates = better quality
    stream: false

  qwen_candidate_4:
    base_url: "http://qwen:8005/v1"
    api_key: "your-secret-api-key-here"
    model: "qwen3-coder-plus"
    stream: false

  # --- OPENROUTER / GROK CANDIDATES ---
  grok_candidate_1:
    base_url: "http://openrouter:8006/api/v1"
    api_key: "your-local-access-key-here"       # OpenRouter proxy key
    model: "x-ai/grok-4-fast:free"              # Free Grok model
    stream: false

  grok_candidate_2:
    base_url: "http://openrouter:8006/api/v1"
    api_key: "your-local-access-key-here"
    model: "x-ai/grok-4-fast:free"
    stream: false

  # --- ADD MORE CANDIDATES ---
  # claude_candidate_1:
  #   base_url: "http://openrouter:8006/api/v1"
  #   api_key: "your-local-access-key-here"
  #   model: "anthropic/claude-3.5-sonnet:free"
  #   stream: false

# =============================================================================
# MOA (MIXTURE-OF-AGENTS) CONFIGURATION
# =============================================================================
moa:
  # --- SINGLE MASTER MODE (backward compatibility) ---
  # Which downstream API to use as the master synthesizer
  master_agent_key: "qwen_candidate_1"

  # --- MULTI-MASTER RACING MODE (NEW) ---
  # Enable racing between multiple master agents for best synthesis
  racing_enabled: true                          # false = use single master only
  racing_runs: 2                                # How many times to run each master

  # List of master agents to race (only used if racing_enabled: true)
  master_agent_keys:
    - "qwen_candidate_1"                        # Fast and good quality
    - "grok_candidate_1"                        # Alternative perspective
    # - "gemini_candidate_1"                    # Add more for comparison

  # --- CANDIDATE GENERATION SETTINGS ---
  num_candidates: 4                             # Total candidates to generate (4-7 recommended)

  # Faster-plus-one mode: request extra candidates, use best N
  faster_plus_one: true                         # Enable reliability boost
  faster_plus_num: 3                            # Request N+3 extra, pick best N

  # Quality control
  minimum_candidates: 3                         # Minimum successful responses before synthesis

  # Logging
  enable_detailed_logging: false                # true = verbose logs (for debugging)

  # Response truncation
  max_candidate_length: 8000                    # Max characters per candidate (prevents token overflow)

# =============================================================================
# TIMEOUT CONFIGURATION
# =============================================================================
timeouts:
  # Soft timeout: proceed with synthesis if minimum_candidates met
  soft_timeout_seconds: 30.0                    # Wait this long for candidates

  # Hard timeout: force stop regardless of candidate count
  hard_timeout_seconds: 120.0                   # Maximum total wait time

  # HTTP client timeout for downstream requests
  http_client_timeout: 180.0                    # Per-request timeout

# =============================================================================
# RETRY CONFIGURATION
# =============================================================================
retry:
  # Retry settings for failed downstream requests
  max_retries: 3                                # How many times to retry
  base_delay: 1.0                               # Initial delay (seconds)
  max_delay: 30.0                               # Maximum delay (seconds)
  jitter_range: 0.2                             # Randomization (0.0-1.0)



# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
server:
  host: "0.0.0.0"       # Bind to all interfaces (use "127.0.0.1" for local only)
  port: 8007            # Port for MOA aggregator service


# =============================================================================
# PROMPTS FOR SYNTHESIS (ADVANCED)
# =============================================================================
# Prompts Configuration

# Prompts Configuration
prompts:
  moa_system_prompt: |
    You are a world-class synthesizer and evaluator. Your role is to analyze multiple, potentially conflicting candidate submissions (inputs) and produce a single, superior, and comprehensive final output. You must reason through the inputs and then provide a clean, well-structured response.

  moa_unified_synthesis_prompt: |
    <task>SYNTHESIS</task>
    <initial_query>{initial_query}</initial_query>
    
    You have been provided with a set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.    
    
    ## Step 1: Quick Candidate Review
    For each candidate, note:
    - Strengths: [what's valuable, accurate, or well-executed]
    - Weaknesses: [what's missing, incorrect, or poorly executed]
    
    ## Step 2: Synthesis Plan
    
    <plan>
      <conflicts>
        [List any contradictions between candidates and how you'll resolve them]
        Rule: Prioritize most supported view; Use strongest evidence or most comprehensive approach
      </conflicts>
      
      <actions>
        1. [What you'll integrate from candidates]
        2. [What you'll enhance or supplement]
        3. [What you'll correct or exclude]
      </actions>
    </plan>
    
    ## Step 3: Final Output
    
    <final_answer>
    [Write your complete, synthesized result here]
    [Must exceed the quality of any individual candidate]
    [Maintain appropriate format for the task type]
    </final_answer>
    
    Quality checklist:
    ✓ Fully addresses the original query or objective
    ✓ More comprehensive and accurate than individual candidates
    ✓ Clear, well-organized, and actionable
    
    # ALERT: The final output MUST be enclosed within <final_answer>...</final_answer> tags for extraction.
    
    # Candidates to evaluate (in no particular order):
    <candidates>{candidates_section}</candidates>
    
    
    # STRICT RULE: Always ensure the final output is well-formatted and adheres to the initial_query requirements 100%.
    YOUR ANSWER WITH MANDATORY <final_answer>...</final_answer> TAGS THAT ENCLOSE THE FINAL OUTPUT:

  candidate_format_template: |
    **Candidate {index}:**
    {content}

    ---

# PROMPT FOR SELECTION INSTEAD OF MERGE
#prompts:
#  moa_system_prompt: |
#    You are a world-class evaluator and selector. Your role is to analyze multiple candidate submissions and identify the single best response based on quality, accuracy, completeness, and adherence to requirements. You will select the winner and output it exactly as written, without any modifications or merging.
#
#  moa_unified_selection_prompt: |
#    <task>SELECTION</task>
#    <initial_query>{initial_query}</initial_query>
#
#    You have been provided with a set of responses from various open-source models to the user query. Your task is to evaluate these responses and select the single best one. You must output the selected response EXACTLY as written, without any modifications, synthesis, or merging. Critical evaluation is essential - recognize biases, errors, incomplete information, and quality differences between candidates.
#
#    ## Step 1: Candidate Evaluation
#    For each candidate, assess:
#    - Accuracy: [factual correctness, reliability of information]
#    - Completeness: [fully addresses query, no missing elements]
#    - Quality: [clarity, structure, professionalism]
#    - Adherence: [follows initial_query requirements and constraints]
#    - Weaknesses: [errors, gaps, or problems]
#
#    ## Step 2: Selection Decision
#
#    <decision>
#      <ranking>
#        [Rank candidates from best to worst with brief justification]
#        1. Candidate X: [why it's the best]
#        2. Candidate Y: [why it's second]
#        ...
#      </ranking>
#
#      <winner>
#        Candidate [X]: [Comprehensive explanation of why this candidate is superior]
#      </winner>
#
#      <rationale>
#        [Detailed reasoning for your selection including:
#         - What makes the winner stand out
#         - Why rejected candidates fell short
#         - How the winner best satisfies the initial_query]
#      </rationale>
#    </decision>
#
#    ## Step 3: Final Output
#
#    <final_answer>
#    [Copy the ENTIRE winning candidate response here EXACTLY as written]
#    [DO NOT modify, edit, enhance, or change ANY part of the selected answer]
#    [DO NOT merge or synthesize - only reproduce the winner verbatim]
#    </final_answer>
#
#    Selection checklist:
#    ✓ Winner fully addresses the original query
#    ✓ Winner is more accurate and complete than other candidates
#    ✓ Winner is copied EXACTLY without modifications
#    ✓ No synthesis or merging has occurred
#
#    # ALERT: The final output MUST be enclosed within <final_answer>...</final_answer> tags for extraction.
#
#    # Candidates to evaluate (in no particular order):
#    <candidates>{candidates_section}</candidates>
#
#    # STRICT RULES:
#    1. Select ONLY ONE candidate as the winner
#    2. Output the winner EXACTLY as written - NO CHANGES ALLOWED
#    3. Do NOT synthesize, merge, or modify the selected answer
#    4. Always use <final_answer>...</final_answer> tags
#
#    YOUR ANSWER WITH MANDATORY <final_answer>...</final_answer> TAGS CONTAINING THE UNMODIFIED WINNER:
#
#  candidate_format_template: |
#    **Candidate {index}:**
#    {content}
#
#    ---

